{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "from pandas import DataFrame\n",
    "import numpy\n",
    "from datetime import date\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CSv\n",
    "pd.read_csv('....csv')\n",
    "\n",
    "# from Excel\n",
    "pd.read_csv('....csv')\n",
    "\n",
    "# from text\n",
    "## this depends\n",
    "with open('test.txt', 'r') as f:\n",
    "    content = f.readlines()\n",
    "    # for every line, do something\n",
    "\n",
    "# from PDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from link\n",
    "\n",
    "# from FTP\n",
    "\n",
    "# from scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SQL server\n",
    "\n",
    "# from Teradata\n",
    "\n",
    "# from Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write\n",
    "df.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "df.columns.tolist()\n",
    "\n",
    "# shape\n",
    "df.shape()\n",
    "\n",
    "# data types\n",
    "df.dtypes\n",
    "\n",
    "# empty cells\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min max median mode \n",
    "df.descibe()\n",
    "\n",
    "# plot distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df.age, bins = 50 )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catagorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names & data type & unique values\n",
    "for column in df.columns:\n",
    "    uniques = sorted(df[column].unique())\n",
    "    print('{0:20s} {1:5d} \\t {2:}'.format(column, len(uniques), df[column].dtype) ,'   ',uniques[:5])\n",
    "    \n",
    "# groupby and count \n",
    "df.groupby('converted').converted.count()\n",
    "df.groupby(['country']).converted.agg(['count','mean'])\n",
    "\n",
    "# A/B tests after grouping\n",
    "from scipy.stats import ttest_ind\n",
    "control = df[df['test'] == 0 ].conversion\n",
    "test = df[df['test'] == 1 ].conversion\n",
    "\n",
    "ttest_ind(control, test,equal_var=False)\n",
    "### The calculated t-statistic. how many sigmas away from the mean\n",
    "### The two-tailed p-value. large--> no difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to datetime\n",
    "newdf.WDate =  pd.to_datetime(newdf.WDate)\n",
    "\n",
    "# missing values\n",
    "\n",
    "## see all the missing values\n",
    "df[df.theColumn.isna()] \n",
    "\n",
    "## fill na\n",
    "df.fillna(method='ffill', )\n",
    "method: {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’}\n",
    "    \n",
    "## put a placehoder\n",
    "df.fillna('None')\n",
    "\n",
    "## remove na\n",
    "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)e)\n",
    "### axis: axis takes int or string value for rows/columns. Input can be 0 or 1 for Integer and ‘index’ or ‘columns’ for String.\n",
    "### how: how takes string value of two kinds only (‘any’ or ‘all’). ‘any’ drops the row/column if ANY value is Null and ‘all’ drops only if ALL values are null.\n",
    "### thresh: thresh takes integer value which tells minimum amount of na values to drop.\n",
    "### subset: It’s an array which limits the dropping process to passed rows/columns through list.\n",
    "### inplace: It is a boolean which makes the changes in data frame itself if True.\n",
    "\n",
    "## imupte with mean/ median \n",
    "df.fillna(df.mean(),inplace=True) \n",
    "df.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "\n",
    "## impute with machine learning KNN\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_filled = imputer.fit_transform(df)\n",
    "\n",
    "# outliers\n",
    "\n",
    "## remove\n",
    "df[(df.age < 80) & (df.age >18)]  \n",
    "\n",
    "df.age.loc[(df.age < 17)|(df.age > 80)] = np.nan\n",
    "df.age = df.age.dropna()\n",
    "## put a cap\n",
    "np.where(df.age > 80,80,df.age)\n",
    "\n",
    "## impute with max\n",
    "\n",
    "\n",
    "## binning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='outer', left_index=True, right_index=True) # same index\n",
    "\n",
    "how:{‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scalar normalize, rescales the values into a range of [0,1],the outliers from the data set are lost.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_all_scaled = scaler.fit_transform(x_train_knn_all)\n",
    "\n",
    "# standerdize,a mean (μ) of 0 and standard deviation (σ) of 1 (unit variance).\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# log scaler\n",
    "\n",
    "# binning\n",
    "cuts = [17, 25, 33, 40, 50, 60, 80] # cut points\n",
    "group_names = ['17-25','25-33','33-40','40-49', '50-59', '60+']\n",
    "\n",
    "df['age_categories'] = pd.cut(df['age'], cuts, labels=group_names)\n",
    "\n",
    "\n",
    "# dummies\n",
    "new_df = pd.get_dummies(df, prefix='age', columns=['age_categories'],drop_first=True)\n",
    "\n",
    "# interation terms\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)\n",
    "train_x = poly.fit_transform(train_x)\n",
    "test_x = poly.fit_transform(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imbalance & train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalance\n",
    "\n",
    "## upsampling Minority Class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=576,    # to match majority class\n",
    "                                 random_state=2019) # reproducible results\n",
    "### Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "## dowmsampling Majority Class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=49,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "### Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "## smote\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "sm = SMOTE(kind='regular',n_jobs=8,k_neighbors=9,m_neighbors=12)\n",
    "train_x, train_y= sm.fit_sample(train_x, train_y)\n",
    "\n",
    "## change metric\n",
    "Area Under ROC Curve (AUROC).\n",
    "\n",
    "## Penalize Algorithms (Cost-Sensitive Training)\n",
    "SVM\n",
    "from sklearn.svm import SVC\n",
    "clf_3 = SVC(kernel='linear', \n",
    "            class_weight='balanced', # penalize\n",
    "            probability=True)\n",
    "\n",
    "## tree based method \n",
    "tree ensembles (Random Forests, Gradient Boosted Trees, etc.)\n",
    "\n",
    "# spilit\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y= train_test_split(data_modeling, data_y, test_size = 0.25, random_state = 2019)\n",
    "\n",
    "# crossvalidation\n",
    "\n",
    "# grid serach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls\n",
    "\n",
    "# lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf\n",
    "\n",
    "# svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foramtting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore wornings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# % indicates an IPython magic function, Set floating point precision for pretty printing.\n",
    "%precision 2 \n",
    "\n",
    "# move plots inline\n",
    "%matplotlib inline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
